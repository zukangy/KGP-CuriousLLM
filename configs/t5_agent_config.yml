root_dir: "."
checkpoint_dir: "checkpoints_t5/"
random_seed: 1028
dataset:
  data_dir: "DATA/T5_traversal_agent/reason_instruction.json"
  train_size: 0.8
  max_source_text_len: 256
  max_target_text_len: 256
model:
  model_name: "google/flan-t5-large"
  from_local_checkpoint: "checkpoints_t5/lora/checkpoint-100"
lora:
  lora_params:
    r: 4
    lora_alpha: 4
    lora_dropout: 0.05
    target_modules: ["q", "v"]
  lora_training_params:
    output_dir: "lora/"
    learning_rate: 0.0001
    num_train_epochs: 3
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    logging_steps: 250
    save_strategy: "steps"
    save_steps: 100
    evaluation_strategy: "steps"
    eval_steps: 3000
    save_total_limit: 5
  save_model_path: "model/"
