root_dir: '.'
seed: 2048
dataset: "hotpot"
question_dataset: "DATA/HotpotQA/indexed_test_docs.json"
device: "mps"
init_retriever:
  name: 'tfidf'
  no_traversal: False
  no_traversal_topk: 7
  topk: 5
retriever:
  name: "bm25"
  model: "models/mistral/mistral_quantized_8_bit"
  adapter: "models/mistral/ft_mistral_quantized_8_bit_2024-04-05_21-28-35/cp_mistral_quantized_8_bit/adapters.npz"
  model_params:
    lora_rank: 32
    lora_layers: 32
  inference_params:
    temp: 0.6
    top_p: 0.85
    max_token_len: 50
  T5_params:
    model_path: "models/checkpoints_t5/reason_t5-large"
    max_source_length: 512
    max_target_length: 512
  traversal_params: 
    n_hop: 2
    n_neighbors: 4
KG: "DATA/KG/graphs/graph_hotpot/graph.gpickle"
emb_model: 
  model: "sentence-transformers/multi-qa-MiniLM-L6-cos-v1"
checkpoint:
  id: "bm25_agent"
  resume: False
  save_every: 1
  save_dir: "DATA/KG/evidence/hotpot_evidence_100"
  checkpoint_path: 